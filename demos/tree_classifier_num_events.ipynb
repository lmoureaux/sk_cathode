{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many signal events do we need?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook studies the SIC achieved by boosted decision trees depending on the number of signal events present in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import subprocess\n",
    "import sys\n",
    "import tqdm\n",
    "from os.path import dirname, join, realpath\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# adding parent directory to path\n",
    "parent_dir = dirname(realpath(globals()[\"_dh\"][0]))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "import sk_cathode\n",
    "from sk_cathode.classifier_models.boosted_decision_tree import HGBClassifier\n",
    "from sk_cathode.utils.ensembling_utils import EnsembleModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same input data as in `demos/weak_supervision.ipynb` are used here. We make use of the same separation into train/validation/test data. However, we don't make use of the extra train/validation signal, as we won't train a supervised classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./input_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation (download and high-level preprocessing)\n",
    "if not (Path(data_path) / \"innerdata_test.npy\").exists():\n",
    "    data_preparation = Path(sk_cathode.__file__).parents[1] / 'demos' / 'utils' / 'data_preparation.py'\n",
    "    process = subprocess.run(f\"{sys.executable} {data_preparation} --outdir {data_path}\", shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "innerdata_train = np.load(join(data_path, \"innerdata_train.npy\"))\n",
    "innerdata_val = np.load(join(data_path, \"innerdata_val.npy\"))\n",
    "innerdata_test = np.load(join(data_path, \"innerdata_test.npy\"))\n",
    "innerdata_extrabkg_train = np.load(join(data_path, \"innerdata_extrabkg_train.npy\"))\n",
    "innerdata_extrabkg_val = np.load(join(data_path, \"innerdata_extrabkg_val.npy\"))\n",
    "innerdata_extrabkg_test = np.load(join(data_path, \"innerdata_extrabkg_test.npy\"))\n",
    "innerdata_extrasig = np.load(join(data_path, \"innerdata_extrasig.npy\"))\n",
    "\n",
    "# Enriching the test set with extra signal.\n",
    "# We could use all, but this way it's consistent with previous notebooks.\n",
    "innerdata_extrasig_test = innerdata_extrasig[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix together train and validation set for different splits\n",
    "innerdata_train_val = np.vstack([innerdata_train, innerdata_val])\n",
    "innerdata_extrabkg_train_val = np.vstack([innerdata_extrabkg_train, innerdata_extrabkg_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick and mix signal events\n",
    "\n",
    "Here we pick how many signal events we want to use. The notebook deals with a single number of events at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_events = 125\n",
    "\n",
    "indices = np.where(innerdata_train_val[:, -1] == 1)[0]\n",
    "print('Available signal events:', len(indices))\n",
    "\n",
    "rng = np.random.default_rng(seed=42)  # For reproducibility\n",
    "rng.shuffle(indices)\n",
    "\n",
    "# Keep only the requested number of signal events\n",
    "selection = np.ones(len(innerdata_train_val), dtype=bool)\n",
    "selection[indices[signal_events:]] = False\n",
    "innerdata_train_val_nsignals = innerdata_train_val[selection]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree definitions\n",
    "\n",
    "We define functions for the BDT-based classification:\n",
    "* `train()` is used to construct the model. It operates on two datasets: the \"data\", which contains a small fraction of signal events, and the \"background\" which contains none.\n",
    "* `get_selection_indices()` is used to select the anomalous items using the fitted model. The dataset passed to this function contains a lot more signal than in `train()`, so the model must not be changed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data: np.array, background: np.array, save_label: str,\n",
    "          n_classifiers:int=10, split_key:int=1337):\n",
    "    \"\"\"\n",
    "    Trains an ensemble of trees with different train/val splits.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.array\n",
    "        Array containing \"data\" features, i.e. a mix of background and signal.\n",
    "    background : np.array\n",
    "        Array containing \"background\" features.\n",
    "    save_label : str\n",
    "        A string inserted in the path when saving the trained models.\n",
    "    n_classifier : int\n",
    "        The number of trees to ensemble.\n",
    "    split_key : int\n",
    "        Used in the initialization of the train/val split. Should be\n",
    "        significantly larger than n_classifiers.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    An ensemble model that can be used to obtain the averaged score for events\n",
    "    in the test set.\n",
    "    \"\"\"\n",
    "\n",
    "    model_list = []\n",
    "    for i in range(n_classifiers):\n",
    "\n",
    "        # different split per classifier\n",
    "        # (could also do this more controlled via a fixed k-folding scheme)\n",
    "        innerdata_train, innerdata_val = train_test_split(\n",
    "            data, train_size=0.8, random_state=split_key+i)\n",
    "        innerdata_extrabkg_train, innerdata_extrabkg_val = train_test_split(\n",
    "            background, train_size=0.8, random_state=split_key+i)\n",
    "\n",
    "        # assigning label 1 to \"data\"\n",
    "        clsf_train_data = innerdata_train\n",
    "        clsf_train_data[:, -1] = np.ones_like(clsf_train_data[:, -1])\n",
    "        clsf_val_data = innerdata_val\n",
    "        clsf_val_data[:, -1] = np.ones_like(clsf_val_data[:, -1])\n",
    "\n",
    "        # and label 0 to background\n",
    "        clsf_train_bkg = innerdata_extrabkg_train\n",
    "        clsf_train_bkg[:, -1] = np.zeros_like(clsf_train_bkg[:, -1])\n",
    "        clsf_val_bkg = innerdata_extrabkg_val\n",
    "        clsf_val_bkg[:, -1] = np.zeros_like(clsf_val_bkg[:, -1])\n",
    "\n",
    "        # mixing together and shuffling\n",
    "        clsf_train_set = np.vstack([clsf_train_data, clsf_train_bkg])\n",
    "        clsf_val_set = np.vstack([clsf_val_data, clsf_val_bkg])\n",
    "        clsf_train_set = shuffle(clsf_train_set, random_state=42)\n",
    "        clsf_val_set = shuffle(clsf_val_set, random_state=42)\n",
    "\n",
    "        # fit scaler\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(clsf_train_set[:, 1:-1])\n",
    "\n",
    "        # train classifier\n",
    "        classifier_savedir = f\"./trained_classifier_idealized-ad_ensemble{save_label}/model_{i}/\"\n",
    "        classifier = HGBClassifier(save_path=classifier_savedir,\n",
    "                                   early_stopping=True, max_iters=None,\n",
    "                                   verbose=False)\n",
    "\n",
    "        # We don't want to overwrite the model if it already exists.\n",
    "        if not (Path(classifier_savedir) / \"CLSF_models\").exists():\n",
    "            X_train = scaler.transform(clsf_train_set[:, 1:-1])\n",
    "            y_train = clsf_train_set[:, -1]\n",
    "            X_val = scaler.transform(clsf_val_set[:, 1:-1])\n",
    "            y_val = clsf_val_set[:, -1]\n",
    "            classifier.fit(X_train, y_train, X_val, y_val)\n",
    "        else:\n",
    "            print(f\"The model exists already in {classifier_savedir}. Remove first if you want to overwrite. Loading its best state now.\")\n",
    "            classifier.load_best_model()\n",
    "\n",
    "        # merge scaler and classifier into a single pipeline model\n",
    "        pipeline = make_pipeline(scaler, classifier)\n",
    "        model_list.append(pipeline)\n",
    "\n",
    "    # Now merging all these models into a single ensemble model\n",
    "    return EnsembleModel(model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selection_indices(model: EnsembleModel, data: np.array, truth: np.array, efficiency=1e-4):\n",
    "    \"\"\"\n",
    "    Returns a boolean array with the indices of \"anomalous\" rows.\n",
    "    \n",
    "    We select the rows with the highest prediction. The threshold is set\n",
    "    according to the *background* selection efficiency (assuming this can be\n",
    "    obtained from sidebands).\n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    model : EnsembleModel\n",
    "        The model to use for the selection.\n",
    "    data : np.array\n",
    "        The features to use to perform the selection.\n",
    "    truth : np.array\n",
    "        Truth labels so we can compute the background efficiency.\n",
    "    efficiency : np.array\n",
    "        The background selection efficiency.\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = model.predict(data).flatten()\n",
    "    threshold = np.quantile(predictions[~truth.astype(bool)], 1 - efficiency)\n",
    "    return predictions > threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models\n",
    "\n",
    "We train 10 classifiers with different initializations so we can check the variance of the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    train(innerdata_train_val_nsignals, innerdata_extrabkg_train_val, f'-{signal_events}_{i}', split_key=(i + 1) * 1337)\n",
    "    for i in tqdm.tqdm(range(10))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "We call `get_selection_indices` for each trained model and compute the SIC, then report the average and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's evaluate the signal extraction performance on the same test set\n",
    "\n",
    "clsf_test_set = np.vstack([innerdata_test,\n",
    "                           innerdata_extrabkg_test,\n",
    "                           innerdata_extrasig_test])\n",
    "\n",
    "X_test = clsf_test_set[:, 1:-1]\n",
    "y_test = clsf_test_set[:, -1]\n",
    "\n",
    "sics = []\n",
    "for ensemble in classifiers:\n",
    "    selection = get_selection_indices(ensemble, X_test, y_test, efficiency=1e-4)\n",
    "    tpr = (y_test[selection]).sum() / y_test.sum()\n",
    "    fpr = (1 - y_test[selection]).sum() / len(y_test)\n",
    "    sics.append(tpr / np.sqrt(fpr))\n",
    "\n",
    "print(f'The SIC we get is: {np.mean(sics):.1f} +/- {np.std(sics):.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the results for varying numbers of signal events used in the training:\n",
    "\n",
    "| <div style=\"min-width:100px\">Signal events | <div style=\"min-width:100px\">$S/\\sqrt B$ | <div style=\"min-width:100px\">SIC $\\pm$ std |\n",
    "|:-------------:|:-----------:|:------------:|\n",
    "| 525 (all)     | 1.8         | $13.1\\pm0.4$ |\n",
    "| 200           | 0.7         | $9.4\\pm0.5$  |\n",
    "| 150           | 0.5         | $4.3\\pm2.4$  |\n",
    "| 125           | 0.44        | $2.2\\pm2.3$  |\n",
    "| 100           | 0.35        | $0.4\\pm0.9$  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(innerdata_train_val_nsignals), len(innerdata_extrabkg_train_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sk_cathode",
   "language": "python",
   "name": "sk_cathode"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
